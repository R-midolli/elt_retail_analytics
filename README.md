# ğŸ›’ ELT Retail Analytics (Postgres + dbt + Power BI)

End-to-end **ELT pipeline** built with a classic analytics stack:

- **Python** for extraction + loading (**Excel â†’ Postgres / raw**)
- **dbt** for transformations and a **Star Schema** (**staging â†’ marts â†’ reporting**)
- **Power BI** for the semantic model and an executive dashboard

---

## ğŸ“Š Power BI Model Preview

> Screenshot of the **star schema** in Power BI (Model view):

![Power BI star schema](powerbi/screenshots/pbi_model_star_schema.png)

> Screenshot of the **DAX measures** list:

![Measures list](powerbi/screenshots/pbi_measures_list.png)

---

## ğŸ§­ Architecture

### High-level flow

```mermaid
flowchart LR
  A["Online Retail II.xlsx"] --> B["Python load to Postgres"]
  B --> C[("Postgres<br/>schema: raw")]
  C --> D["dbt staging"]
  D --> E["dbt marts<br/>(star schema)"]
  E --> F["dbt reporting<br/>(KPIs / views)"]
  F --> G["Power BI semantic model"]
  G --> H["Executive dashboard"]
````

### Postgres schemas used in this project

* **raw**: source table loaded by Python (`raw.sales`)
* **staging**: dbt staging models (e.g., `staging.stg_sales`)
* **marts**: star-schema tables (e.g., `marts.fact_sales_star`, `marts.dim_*`)
* **analytics_reporting**: KPI tables / reporting views (e.g., `analytics_reporting.kpi_daily`)

---

## ğŸ“Œ Dataset

This project uses the **Online Retail II** dataset (Excel file) stored in:

* `data/raw/online_retail_II.xlsx`

The dataset contains historical e-commerce transactions (2009â€“2011) across multiple countries.
Reference (UCI):

```text
https://archive.ics.uci.edu/ml/datasets/online%2Bretail%2BII
```

---

## ğŸ§± Star Schema (dbt marts)

### Fact table

* **`marts.fact_sales_star`**

  * **Grain:** invoice line (`invoice_no Ã— stock_code Ã— sales_date Ã— customer_id`)
  * **Measures:** `quantity`, `unit_price`, `line_amount`
  * **Keys:** `customer_id`, `sales_date`, `stock_code`, `invoice_no`
  * **Flags:** `is_cancelled`

### Dimensions

* **`marts.dim_date`**

  * **PK:** `date_day`
  * **Attributes (examples):** `year`, `month`, `dow`, `iso_week`, `month_start`

* **`marts.dim_products`**

  * **PK:** `stock_code`
  * **Attributes:** `product_description`, `is_product`

* **`marts.dim_customers`**

  * **PK:** `customer_id`
  * **Attributes:** `country`

* **`marts.dim_invoice`**

  * **PK:** `invoice_no`
  * **Attributes:** `invoice_day`, `invoice_ts`, `is_cancelled`

---

## ğŸ•¸ï¸ dbt Lineage Graph

The lineage graph screenshot is stored at:

* `assets/images/dbt_graph.png`

![dbt graph](assets/images/dbt_graph.png)

### Regenerate dbt docs + lineage

```bash
cd dbt_retail

# load env vars for dbt connection
set -o allexport; . ../.env; set +o allexport

uv run dbt docs generate --profiles-dir . --no-partial-parse
uv run dbt docs serve --profiles-dir . --port 8080
# open http://localhost:8080 â†’ Lineage Graph â†’ take a screenshot
```

---

## ğŸ—‚ï¸ dbt Models (overview)

### Source

* `source('raw_retail', 'sales')` â†’ table **`raw.sales`**

Source definition is in:

* `dbt_retail/models/staging/sources.yml`

### Staging

* **`staging.stg_sales`**

  * Standardizes types and column names
  * Computes `sales_date` and `line_amount`
  * Filters obvious invalid records (e.g., null identifiers, non-positive values)

### Marts

Core marts (star schema):

* `marts.fact_sales`
* `marts.fact_sales_star`
* `marts.dim_date`
* `marts.dim_products`
* `marts.dim_invoice`
* `marts.dim_customers`

### Reporting

* `analytics_reporting.kpi_daily`
* `analytics_reporting.kpi_product`
* `analytics_reporting.kpi_customer`
* `analytics_reporting.sales_star` (flattened view for exploration)
* Exposure: `retail_kpi_dashboard`

---

## ğŸ“‚ Project Structure

```text
ELT_retail_analytics/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â””â”€â”€ online_retail_II.xlsx
â”‚   â””â”€â”€ processed/                 # optional local exports (not required by pipeline)
â”œâ”€â”€ src/
â”‚   â””â”€â”€ load_raw_online_retail.py  # core loader logic
â”œâ”€â”€ elt_step1_extract.py           # pipeline step: Excel â†’ Postgres/raw
â”œâ”€â”€ sql/
â”‚   â””â”€â”€ init.sql                   # schema/bootstrap SQL
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ dbt_retail/
â”‚   â”œâ”€â”€ dbt_project.yml
â”‚   â”œâ”€â”€ profiles.yml
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/
â”‚   â”‚   â”‚   â”œâ”€â”€ sources.yml
â”‚   â”‚   â”‚   â””â”€â”€ stg_sales.sql
â”‚   â”‚   â”œâ”€â”€ marts/core/
â”‚   â”‚   â””â”€â”€ reporting/
â”‚   â””â”€â”€ target/                    # generated by dbt docs/compile
â”œâ”€â”€ powerbi/
â”‚   â”œâ”€â”€ retail_analytics_exec_dashboard.pbix
â”‚   â”œâ”€â”€ elt_retail_analytics.pbix
â”‚   â””â”€â”€ screenshots/
â”‚       â”œâ”€â”€ pbi_model_star_schema.png
â”‚       â””â”€â”€ pbi_measures_list.png
â””â”€â”€ assets/
    â””â”€â”€ images/
        â””â”€â”€ dbt_graph.png
```

---

## âœ… Prerequisites

* **Docker + Docker Compose** (Postgres)
* **Python 3.10+**
* **uv** (Python package manager)
* **Power BI Desktop** (for the BI part)

---

## âš™ï¸ Setup

### 1) Create your `.env`

```bash
cp .env.example .env
```

Example:

```env
POSTGRES_USER=retail_user
POSTGRES_PASSWORD=retail_pass
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=retail
```

### 2) Start Postgres (Docker)

```bash
docker compose up -d
docker ps
```

### 3) Initialize schemas

```bash
docker exec -it retail_pg psql -U retail_user -d retail -f /sql/init.sql
```

---

## ğŸš€ Run the pipeline

### Step 1 â€” Extract + Load (Python â†’ Postgres/raw)

From repository root:

```bash
uv sync
uv run python elt_step1_extract.py
```

What it does:

* Reads `data/raw/online_retail_II.xlsx`
* Cleans obvious invalid rows (null IDs, negative/zero quantities or prices)
* Loads to **`raw.sales`** in Postgres

### Step 2 â€” Transform (dbt)

```bash
cd dbt_retail
set -o allexport; . ../.env; set +o allexport

uv run dbt run --profiles-dir .
```

Build a specific subset (example):

```bash
uv run dbt run --profiles-dir . --select stg_sales fact_sales fact_sales_star dim_date dim_products dim_invoice dim_customers
```

### Step 3 â€” Test (dbt)

```bash
uv run dbt test --profiles-dir .
```

---

## ğŸ“Š Power BI â€” Semantic model & dashboard

### 1) Connect to Postgres

In Power BI Desktop:

* **Get Data â†’ PostgreSQL database**
* Server: `localhost`
* Database: `retail`
* Credentials: from `.env`

Load tables from these schemas:

* `marts`: `fact_sales_star`, `dim_date`, `dim_customers`, `dim_products`, `dim_invoice`
* (optional) `analytics_reporting`: `kpi_daily`, `kpi_product`, `kpi_customer`, `sales_star`

### 2) Relationships (Model view)

Create active relationships (Many-to-one, Single direction):

* `fact_sales_star[customer_id]` â†’ `dim_customers[customer_id]`
* `fact_sales_star[sales_date]` â†’ `dim_date[date_day]`
* `fact_sales_star[stock_code]` â†’ `dim_products[stock_code]`
* `fact_sales_star[invoice_no]` â†’ `dim_invoice[invoice_no]`

---

## ğŸ§® DAX Measures (copy/paste)

In the provided PBIX, table names may appear with a schema prefix (example: `marts_fact_sales_star`).
If your model uses different names, replace them in the measures below.

```DAX
-- Revenue
CA =
SUM ( 'marts_fact_sales_star'[line_amount] )

-- Orders / Invoices
Commandes (Invoices) =
DISTINCTCOUNT ( 'marts_fact_sales_star'[invoice_no] )

-- Unique customers
Clients =
DISTINCTCOUNT ( 'marts_fact_sales_star'[customer_id] )

-- Units sold
UnitÃ©s =
SUM ( 'marts_fact_sales_star'[quantity] )

-- Average order value
AOV (â‚¬ / commande) =
DIVIDE ( [CA], [Commandes (Invoices)] )

-- Average selling price
ASP (â‚¬ / unitÃ©) =
DIVIDE ( [CA], [UnitÃ©s] )

-- Revenue per customer
CA / client =
DIVIDE ( [CA], [Clients] )

-- Distinct products
Produits distincts =
DISTINCTCOUNT ( 'marts_fact_sales_star'[stock_code] )

-- Units per order
UnitÃ©s / commande =
DIVIDE ( [UnitÃ©s], [Commandes (Invoices)] )

-- Cancellation rate (by invoices)
Taux d'annulation =
DIVIDE (
    CALCULATE (
        DISTINCTCOUNT ( 'marts_fact_sales_star'[invoice_no] ),
        'marts_fact_sales_star'[is_cancelled] = TRUE ()
    ),
    [Commandes (Invoices)]
)

-- Previous month revenue
CA Mois prÃ©cÃ©dent =
CALCULATE ( [CA], DATEADD ( 'marts_dim_date'[date_day], -1, MONTH ) )

-- Month-over-month change (%)
CA MoM % =
DIVIDE ( [CA] - [CA Mois prÃ©cÃ©dent], [CA Mois prÃ©cÃ©dent] )
```

---

## ğŸ” Refresh notes

After running dbt:

* In Power BI Desktop: **Home â†’ Refresh**
* If columns changed: **Transform data â†’ Refresh Preview â†’ Close & Apply**

---

## ğŸ§ª Troubleshooting

### dbt is not found

Use the project runner consistently:

```bash
cd dbt_retail
uv run dbt --version
```

### Verify data in Postgres

```sql
SELECT COUNT(*) FROM raw.sales;
SELECT SUM(line_amount) FROM marts.fact_sales_star;
```

---

## ğŸ“Œ Tech stack

* **Source:** Online Retail II (Excel)
* **ELT:** Python + Postgres (Docker)
* **Transformations:** dbt
* **BI:** Power BI Desktop
* **Version control:** Git + GitHub

---

## ğŸ§­ Roadmap (optional)

* Add CI (GitHub Actions) for `dbt build` + `dbt test`
* Add incremental models for larger datasets
* Add automated refresh script for local runs
* Extend time-intelligence measures (YoY, rolling windows)

---

## ğŸ“ Attribution

Dataset reference:

```text
https://archive.ics.uci.edu/ml/datasets/online%2Bretail%2BII
```

Author: Rafael Midolli
